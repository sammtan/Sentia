{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f61290",
   "metadata": {},
   "source": [
    "# ğŸ§  Sentium â€” Colab Training Notebook\n",
    "**Samuel Tanaka Â· Universitas Indonesia Â· February 2026**\n",
    "\n",
    "This notebook trains the **Sentium Phase 0 baseline (454 M params)** on Google Colab using your GPU allocation, with all checkpoints and datasets living in your **2 TB Google Drive**.\n",
    "\n",
    "### Workflow\n",
    "```\n",
    "GitHub repo (code)  â”€â”€â–º  Colab VM (compute)  â”€â”€â–º  Google Drive (data & checkpoints)\n",
    "      â–²                                                      â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€ push from VS Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                           resume next session â—„â”€â”˜\n",
    "```\n",
    "\n",
    "### Sections\n",
    "1. Mount Google Drive\n",
    "2. GPU / runtime check\n",
    "3. Install dependencies\n",
    "4. Clone / pull repo from GitHub\n",
    "5. Configure Drive paths\n",
    "6. Load dataset (HuggingFace streaming â†’ Drive cache)\n",
    "7. Build model\n",
    "8. Configure checkpoint saving to Drive\n",
    "9. Train\n",
    "10. Resume training from checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ce08e",
   "metadata": {},
   "source": [
    "## 1 Â· Mount Google Drive\n",
    "Your 2 TB Drive is the persistent storage layer â€” checkpoints survive session restarts, and the dataset cache means you won't re-download every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ede361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "âœ… Drive mounted. Working root: /content/drive/MyDrive/Sentium\n",
      "   Free space: âœ… Drive mounted. Working root: /content/drive/MyDrive/Sentium\n",
      "   Free space: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "import os\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/Sentium\"\n",
    "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "print(f\"âœ… Drive mounted. Working root: {DRIVE_ROOT}\")\n",
    "print(f\"   Free space: \", end=\"\")\n",
    "os.system(f\"df -h /content/drive | tail -1 | awk '{{print $4}}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d628576",
   "metadata": {},
   "source": [
    "## 2 Â· GPU / Runtime Check\n",
    "Verify what hardware Colab allocated. **If you see CPU here, go to Runtime â†’ Change runtime type â†’ GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6293ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU: Tesla T4\n",
      "   VRAM: 14.6 GB\n",
      "   CUDA: 12.4\n",
      "   cuDNN: 90300\n",
      "   Tier: unknown / limited\n",
      "Tesla T4, 15360 MiB, 14910 MiB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    vram_gb = props.total_memory / 1024**3\n",
    "    print(f\"âœ… GPU: {props.name}\")\n",
    "    print(f\"   VRAM: {vram_gb:.1f} GB\")\n",
    "    print(f\"   CUDA: {torch.version.cuda}\")\n",
    "    print(f\"   cuDNN: {torch.backends.cudnn.version()}\")\n",
    "    # Colab GPU tier suggestion\n",
    "    if vram_gb >= 40:\n",
    "        tier = \"A100 ğŸš€  (best tier â€” batch=16, no grad-ckpt)\"\n",
    "    elif vram_gb >= 22:\n",
    "        tier = \"L4 âš¡  (great â€” batch=8)\"\n",
    "    elif vram_gb >= 15:\n",
    "        tier = \"T4 âœ”  (standard â€” batch=4)\"\n",
    "    else:\n",
    "        tier = \"unknown / limited\"\n",
    "    print(f\"   Tier: {tier}\")\n",
    "else:\n",
    "    print(\"âŒ No GPU detected. Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader 2>/dev/null || echo \"(nvidia-smi not available)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d53fa",
   "metadata": {},
   "source": [
    "## 3 Â· Install Dependencies\n",
    "`transformers` and `datasets` are needed for real corpus training. `wandb` is optional but recommended for loss curves you can view from your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4d84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "# PyTorch is pre-installed on Colab â€” only install the extras\n",
    "!pip install -q \\\n",
    "    einops>=0.7.0 \\\n",
    "    transformers>=4.40.0 \\\n",
    "    datasets>=2.18.0 \\\n",
    "    tokenizers>=0.19.0 \\\n",
    "    wandb>=0.17.0 \\\n",
    "    tqdm>=4.66.0 \\\n",
    "    scipy>=1.12.0\n",
    "\n",
    "print(\"âœ… Dependencies installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19449219",
   "metadata": {},
   "source": [
    "## 4 Â· Clone / Update Repository from GitHub\n",
    "The code lives on GitHub. Every time you open a new Colab session, this cell pulls the latest version â€” so your edits from VS Code are instantly available here after a `git push`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4b7db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Cloning https://github.com/sammtan/Sentia.git ...\n",
      "Cloning into '/content/Sentia'...\n",
      "remote: Enumerating objects: 56, done.\u001b[K\n",
      "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
      "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
      "remote: Enumerating objects: 56, done.\u001b[K\n",
      "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
      "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
      "remote: Total 56 (delta 4), reused 56 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (56/56), 5.36 MiB | 11.94 MiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n",
      "remote: Total 56 (delta 4), reused 56 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (56/56), 5.36 MiB | 11.94 MiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 76, in resolve\n",
      "    collected = self.factory.collect_root_requirements(root_reqs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 538, in collect_root_requirements\n",
      "    reqs = list(\n",
      "           ^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 494, in _make_requirements_from_install_req\n",
      "    cand = self._make_base_candidate_from_link(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 210, in _make_base_candidate_from_link\n",
      "    self._editable_candidate_cache[link] = EditableCandidate(\n",
      "                                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 328, in __init__\n",
      "    super().__init__(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n",
      "    self.dist = self._prepare()\n",
      "                ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 235, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 338, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_editable_requirement(self._ireq)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 698, in prepare_editable_requirement\n",
      "    dist = _get_prepared_distribution(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 72, in _get_prepared_distribution\n",
      "    abstract_dist.prepare_distribution_metadata(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/distributions/sdist.py\", line 54, in prepare_distribution_metadata\n",
      "    self.req.isolated_editable_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_install.py\", line 540, in isolated_editable_sanity_check\n",
      "    and not self.supports_pyproject_editable\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/functools.py\", line 1001, in __get__\n",
      "    val = self.func(instance)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_install.py\", line 257, in supports_pyproject_editable\n",
      "    return \"build_editable\" in self.pep517_backend._supported_features()\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 153, in _supported_features\n",
      "    return self._call_hook('_supported_features', {})\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 321, in _call_hook\n",
      "    raise BackendUnavailable(data.get('traceback', ''))\n",
      "pip._vendor.pyproject_hooks._impl.BackendUnavailable: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 77, in _build_backend\n",
      "    obj = import_module(mod_path)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1126, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1140, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'setuptools.backends'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 76, in resolve\n",
      "    collected = self.factory.collect_root_requirements(root_reqs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 538, in collect_root_requirements\n",
      "    reqs = list(\n",
      "           ^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 494, in _make_requirements_from_install_req\n",
      "    cand = self._make_base_candidate_from_link(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 210, in _make_base_candidate_from_link\n",
      "    self._editable_candidate_cache[link] = EditableCandidate(\n",
      "                                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 328, in __init__\n",
      "    super().__init__(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n",
      "    self.dist = self._prepare()\n",
      "                ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 235, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 338, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_editable_requirement(self._ireq)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 698, in prepare_editable_requirement\n",
      "    dist = _get_prepared_distribution(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 72, in _get_prepared_distribution\n",
      "    abstract_dist.prepare_distribution_metadata(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/distributions/sdist.py\", line 54, in prepare_distribution_metadata\n",
      "    self.req.isolated_editable_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_install.py\", line 540, in isolated_editable_sanity_check\n",
      "    and not self.supports_pyproject_editable\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/functools.py\", line 1001, in __get__\n",
      "    val = self.func(instance)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_install.py\", line 257, in supports_pyproject_editable\n",
      "    return \"build_editable\" in self.pep517_backend._supported_features()\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 153, in _supported_features\n",
      "    return self._call_hook('_supported_features', {})\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 321, in _call_hook\n",
      "    raise BackendUnavailable(data.get('traceback', ''))\n",
      "pip._vendor.pyproject_hooks._impl.BackendUnavailable: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 77, in _build_backend\n",
      "    obj = import_module(mod_path)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1126, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1140, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'setuptools.backends'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mâœ… Repo ready at /content/Sentia\n",
      "\u001b[33ma1ee999\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m chore: tighten .gitignore, untrack binaries and personal notes\n",
      "\u001b[33mb50b2ab\u001b[m feat: Colab training integration  train_colab.py + sentium_colab.ipynb with Drive sync\n",
      "\u001b[33m2899201\u001b[m feat: initial Sentium project  Phase 0 baseline, paper, tests (26/26 pass)\n",
      "âœ… Repo ready at /content/Sentia\n",
      "\u001b[33ma1ee999\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m chore: tighten .gitignore, untrack binaries and personal notes\n",
      "\u001b[33mb50b2ab\u001b[m feat: Colab training integration  train_colab.py + sentium_colab.ipynb with Drive sync\n",
      "\u001b[33m2899201\u001b[m feat: initial Sentium project  Phase 0 baseline, paper, tests (26/26 pass)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# â”€â”€â”€ CONFIGURE THIS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GITHUB_USER = \"sammtan\"   # â† change this\n",
    "REPO_NAME   = \"Sentia\"                 # â† change if your repo name differs\n",
    "BRANCH      = \"master\"\n",
    "# If repo is private, use a Personal Access Token (PAT):\n",
    "#   GITHUB_TOKEN = \"ghp_xxxxxxxxxxxx\"\n",
    "#   REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git\"\n",
    "REPO_URL    = f\"https://github.com/{GITHUB_USER}/{REPO_NAME}.git\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "REPO_DIR = f\"/content/{REPO_NAME}\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"ğŸ“¦ Repo already cloned â€” pulling latest changes from '{BRANCH}'...\")\n",
    "    !git -C {REPO_DIR} fetch origin\n",
    "    !git -C {REPO_DIR} reset --hard origin/{BRANCH}\n",
    "else:\n",
    "    print(f\"ğŸ“¦ Cloning {REPO_URL} ...\")\n",
    "    !git clone --branch {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "\n",
    "# Install the sentium package in editable mode\n",
    "!pip install -q -e {REPO_DIR}[train]\n",
    "\n",
    "# Add repo to path so imports work\n",
    "import sys\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(f\"âœ… Repo ready at {REPO_DIR}\")\n",
    "!git -C {REPO_DIR} log --oneline -3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40286b4e",
   "metadata": {},
   "source": [
    "## 5 Â· Configure Drive Storage Paths\n",
    "All persistent data (datasets cache, checkpoints, logs) goes into **MyDrive/Sentium/** â€” survives session disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€ All persistent storage lives here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DRIVE_ROOT       = Path(\"/content/drive/MyDrive/Sentium\")\n",
    "DRIVE_CKPT_DIR   = DRIVE_ROOT / \"checkpoints\"      # .pt files\n",
    "DRIVE_CACHE_DIR  = DRIVE_ROOT / \"datasets_cache\"   # HuggingFace cache\n",
    "DRIVE_LOG_DIR    = DRIVE_ROOT / \"logs\"             # training logs\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Local (VM) checkpoint dir â€” synced to Drive after each save\n",
    "LOCAL_CKPT_DIR   = Path(\"/content/checkpoints\")\n",
    "\n",
    "for d in [DRIVE_CKPT_DIR, DRIVE_CACHE_DIR, DRIVE_LOG_DIR, LOCAL_CKPT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Drive paths configured:\")\n",
    "for name, path in {\n",
    "    \"Checkpoints\": DRIVE_CKPT_DIR,\n",
    "    \"Dataset cache\": DRIVE_CACHE_DIR,\n",
    "    \"Logs\": DRIVE_LOG_DIR,\n",
    "}.items():\n",
    "    size = sum(f.stat().st_size for f in path.rglob(\"*\") if f.is_file())\n",
    "    print(f\"  {name:16s}: {path}   ({size/1024**3:.2f} GB used)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac67e83",
   "metadata": {},
   "source": [
    "## 6 Â· Load Dataset from Drive Cache\n",
    "Uses HuggingFace `datasets` in **streaming mode** â€” data flows token-by-token from the internet and is cached on Drive so subsequent sessions are instant.\n",
    "\n",
    "> **Switch dataset** by changing `DATASET_ID` below. Options: `slim_pajama`, `the_pile`, `openwebtext`, `wikitext`, or any HuggingFace dataset id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668efec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)  # ensure sentium is importable\n",
    "\n",
    "from train_colab import HFTextDataset, RandomTokenDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# â”€â”€â”€ Dataset config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATASET_ID     = \"slim_pajama\"  # slim_pajama | the_pile | openwebtext | wikitext\n",
    "TOKENIZER_NAME = \"gpt2\"\n",
    "CONTEXT_LEN    = 512            # will be overridden by train_cfg below\n",
    "MAX_SAMPLES    = None           # set an int (e.g. 50_000) to cap stream for testing\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(f\"Loading dataset: {DATASET_ID}\")\n",
    "print(f\"Tokenizer:       {TOKENIZER_NAME}\")\n",
    "print(f\"Cache dir:       {DRIVE_CACHE_DIR}\")\n",
    "print(\"(First run downloads and caches â€” subsequent runs load from Drive instantly)\")\n",
    "# Dataset is created lazily (streaming) â€” actual loading starts during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcb651",
   "metadata": {},
   "source": [
    "## 7 Â· Build Model\n",
    "Auto-selects VRAM-safe `TrainConfig` for whatever GPU Colab gave you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac8604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentium import Sentium, SentiumConfig\n",
    "from sentium.train import TrainConfig\n",
    "from train_colab import detect_gpu_vram, vram_safe_train_cfg\n",
    "\n",
    "# â”€â”€ Auto-detect VRAM and pick matching config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "detected_vram = detect_gpu_vram()\n",
    "\n",
    "# â”€â”€â”€ Training config (auto-selected; override below if needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_cfg = vram_safe_train_cfg(\n",
    "    vram_gb=detected_vram,\n",
    "    save_dir=str(LOCAL_CKPT_DIR),\n",
    "    wandb=False,            # set True after wandb.login() below\n",
    ")\n",
    "# â”€â”€ Optional manual overrides â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# train_cfg.max_steps      = 200_000\n",
    "# train_cfg.batch_size     = 8\n",
    "# train_cfg.context_end    = 2048\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Match dataset context length to training config\n",
    "CONTEXT_LEN = train_cfg.context_start\n",
    "\n",
    "# â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model_cfg = SentiumConfig.baseline_200m()\n",
    "model     = Sentium(model_cfg)\n",
    "\n",
    "params_m = model.num_parameters() / 1e6\n",
    "print(f\"âœ… Model: {model_cfg.model_name}  â€”  {params_m:.1f} M parameters\")\n",
    "print(f\"   d_model={model_cfg.d_model}, n_layers={model_cfg.n_layers}, \"\n",
    "      f\"n_heads={model_cfg.n_heads}, d_ff={model_cfg.d_ff}\")\n",
    "print(f\"\\nTraining config (VRAM={detected_vram:.1f} GB):\")\n",
    "print(f\"  batch_size            = {train_cfg.batch_size}\")\n",
    "print(f\"  grad_accum            = {train_cfg.grad_accum}\")\n",
    "print(f\"  effective_batch       = {train_cfg.batch_size * train_cfg.grad_accum}\")\n",
    "print(f\"  context_start         = {train_cfg.context_start}\")\n",
    "print(f\"  context_end           = {train_cfg.context_end}\")\n",
    "print(f\"  gradient_checkpointing= {train_cfg.gradient_checkpointing}\")\n",
    "print(f\"  dtype                 = {train_cfg.dtype}\")\n",
    "print(f\"  max_steps             = {train_cfg.max_steps:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fdc65",
   "metadata": {},
   "source": [
    "## 8 Â· Configure Checkpointing to Drive\n",
    "Every `save_every` steps, a `.pt` file is written locally **and immediately copied to Drive**. The Drive copy is the ground truth â€” if Colab disconnects mid-run, you lose at most one interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_colab import patch_trainer_drive_sync, latest_checkpoint\n",
    "from sentium.train import Trainer\n",
    "\n",
    "# â”€â”€â”€ Checkpoint interval (steps) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_cfg.save_every  = 500    # save local + Drive every 500 steps\n",
    "train_cfg.keep_last_n = 5      # keep 5 most recent checkpoints on Drive\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Build dataset + dataloader (streaming, Drive-cached)\n",
    "dataset = HFTextDataset(\n",
    "    dataset_id=DATASET_ID,\n",
    "    seq_len=CONTEXT_LEN,\n",
    "    cache_dir=str(DRIVE_CACHE_DIR),\n",
    "    tokenizer_name=TOKENIZER_NAME,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Build trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=loader,\n",
    "    model_config=model_cfg,\n",
    "    train_config=train_cfg,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Patch Drive sync into every save call\n",
    "patch_trainer_drive_sync(trainer, DRIVE_CKPT_DIR)\n",
    "\n",
    "print(f\"âœ… Trainer ready.\")\n",
    "print(f\"   Checkpoint every {train_cfg.save_every} steps â†’ {DRIVE_CKPT_DIR}\")\n",
    "print(f\"   Keeping last {train_cfg.keep_last_n} checkpoints\")\n",
    "\n",
    "# List existing checkpoints on Drive\n",
    "existing = sorted(DRIVE_CKPT_DIR.glob(\"sentium_step*.pt\"))\n",
    "if existing:\n",
    "    print(f\"\\n   Existing Drive checkpoints ({len(existing)}):\")\n",
    "    for p in existing[-5:]:\n",
    "        size_mb = p.stat().st_size / 1024**2\n",
    "        print(f\"     {p.name}  ({size_mb:.0f} MB)\")\n",
    "else:\n",
    "    print(\"\\n   No existing checkpoints on Drive â€” fresh start.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b1355",
   "metadata": {},
   "source": [
    "## 9 Â· Train ğŸš€\n",
    "Run the cell below to start training. Progress logs are printed to this cell **and** appended to `MyDrive/Sentium/logs/sentium_train.log`.\n",
    "\n",
    "> âš ï¸ Colab free tier disconnects after ~90 min idle or ~12 h. The checkpoint synced to Drive on the last `save_every` boundary is fully resumable â€” see Section 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be311a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys, time\n",
    "\n",
    "# Route trainer prints through logging so they appear in both cell output and log file\n",
    "log_path = DRIVE_LOG_DIR / \"sentium_train.log\"\n",
    "handlers = [logging.StreamHandler(sys.stdout),\n",
    "            logging.FileHandler(log_path, mode=\"a\")]\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s | %(message)s\",\n",
    "                    datefmt=\"%H:%M:%S\",\n",
    "                    handlers=handlers, force=True)\n",
    "\n",
    "print(f\"ğŸ“ Log file: {log_path}\")\n",
    "print(f\"ğŸš€ Starting training â€” {model.num_parameters()/1e6:.1f} M params on {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   Max steps: {train_cfg.max_steps:,}  |  Checkpoint every: {train_cfg.save_every}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "t_start = time.time()\n",
    "trainer.train()\n",
    "elapsed = (time.time() - t_start) / 60\n",
    "print(f\"\\nâœ… Training complete in {elapsed:.1f} min. Final step: {trainer.step}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a292a",
   "metadata": {},
   "source": [
    "## 10 Â· Resume Training from Drive Checkpoint\n",
    "Session expired? New Colab session? Run cells 1â€“7 to re-mount Drive, re-clone, re-build model â€” then run this cell to load the latest checkpoint and continue exactly where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2dd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_colab import latest_checkpoint\n",
    "\n",
    "# â”€â”€ Find latest checkpoint on Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ckpt_path = latest_checkpoint(DRIVE_CKPT_DIR)\n",
    "\n",
    "if ckpt_path is None:\n",
    "    print(\"âŒ No checkpoint found in Drive. Run Section 9 first.\")\n",
    "else:\n",
    "    size_mb = ckpt_path.stat().st_size / 1024**2\n",
    "    print(f\"âœ… Found checkpoint: {ckpt_path.name}  ({size_mb:.0f} MB)\")\n",
    "\n",
    "    # Load into trainer (restores model weights, optimizer state, step counter)\n",
    "    trainer.load_checkpoint(ckpt_path)\n",
    "    print(f\"   Resumed from step {trainer.step:,}\")\n",
    "    print(f\"   Remaining steps: {train_cfg.max_steps - trainer.step:,}\")\n",
    "\n",
    "    # â”€â”€ Continue training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nğŸš€ Resuming training...\")\n",
    "    trainer.train()\n",
    "    print(f\"\\nâœ… Done. Final step: {trainer.step:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2d20f",
   "metadata": {},
   "source": [
    "---\n",
    "## 11 Â· (Optional) Weights & Biases â€” Monitor Loss from Your Laptop\n",
    "W&B lets you watch training curves in real-time from any browser, even while Colab is running on Google's machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Login once â€” stores key in Colab's secret manager for the session\n",
    "wandb.login()   # will prompt for API key; get one free at wandb.ai\n",
    "\n",
    "# Then enable W&B in train_cfg and re-run Section 8+9:\n",
    "train_cfg.use_wandb   = True\n",
    "train_cfg.project_name = \"sentium\"\n",
    "print(\"âœ… W&B enabled. Training metrics will stream to wandb.ai/your-username/sentium\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c37bb",
   "metadata": {},
   "source": [
    "---\n",
    "## 12 Â· (Optional) Smoke Test â€” Verify Everything Works Before Full Run\n",
    "Runs 50 steps on random data with a tiny model. Should complete in < 30 seconds on any GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0456ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {REPO_DIR}/train_colab.py \\\n",
    "    --smoke-test \\\n",
    "    --drive-dir \"{DRIVE_ROOT}\" \\\n",
    "    --no-drive\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

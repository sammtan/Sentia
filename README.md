# SENTIUM

**Structured Entropic Neural Transport with Integral Unified Manifold**

> A next-generation AI architecture: from token sequence processor to geometric, stochastic, operator-based reasoning system.

---

## Vision

Sentium transforms the Transformer from a flat sequence processor into a **Differentiable Geometric Computational Field** â€” natively handling million-token contexts, massive code repositories, and multimodal inputs, while remaining real-time deployable on high-end desktop hardware.

---

## Architecture Overview

```
Structured Input (Text / Code / Multimodal)
â†“
AST-aware / Structured Tokenization
â†“
Geometric Memory Embedding       â† Phase 1
â†“
Integral Operator Attention      â† Phase 1
â†“
Optimal Transport Expert Routing â† Phase 2
â†“
Adaptive Stochastic Depth        â† Phase 3
â†“
Dual Output: Semantic Response + Symbolic Trace
```

---

## Phases

| Phase | Components                                                           | Status               |
| ----- | -------------------------------------------------------------------- | -------------------- |
| 0     | Baseline 200M Transformer (standard MHA, SwiGLU, RoPE, GQA)          | âœ… Implemented        |
| 1     | Geometric Memory (Hyperbolic + Graph) + Operator Attention (NystrÃ¶m) | âœ… Implemented (stub) |
| 2     | Optimal Transport MoE Routing (Sinkhorn)                             | âœ… Implemented        |
| 3     | Adaptive Stochastic Depth (Neural SDE, DropPath)                     | âœ… Implemented        |
| 4     | Neuro-Symbolic dual channel, AST-aware tokenization                  | ğŸ”œ Planned            |

---

## Project Structure

```
sentium/
â”œâ”€â”€ config.py           â† SentiumConfig: all hyperparameters & phase flags
â”œâ”€â”€ tokenizer.py        â† HF tokenizer wrapper + AST-aware hook (Phase 4)
â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ embedding.py    â† EuclideanEmbedding / GeometricEmbedding + RoPE
â”‚   â”œâ”€â”€ attention.py    â† StandardMHA / OperatorAttention (NystrÃ¶m)
â”‚   â”œâ”€â”€ feedforward.py  â† SwiGLUFFN / MoEFFN + OTRouter (Sinkhorn)
â”‚   â”œâ”€â”€ normalization.pyâ† RMSNorm / LayerNorm
â”‚   â””â”€â”€ layer.py        â† SentiumLayer (pre-norm block + stochastic depth)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ baseline.py     â† Sentium (full model, all phases in one class)
â”‚
â”œâ”€â”€ ops/
â”‚   â”œâ”€â”€ nystrom.py      â† Standalone NystrÃ¶m attention kernel
â”‚   â””â”€â”€ sinkhorn.py     â† Standalone Sinkhorn OT routing
â”‚
â”œâ”€â”€ train/
â”‚   â””â”€â”€ trainer.py      â† Training loop (AMP, grad clip, curriculum, W&B)
â”‚
â””â”€â”€ eval/
    â””â”€â”€ benchmark.py    â† Perplexity / latency / context scaling benchmarks

tests/
â””â”€â”€ test_model.py       â† Full unit test suite

train_baseline.py       â† Entry-point training script
pyproject.toml
requirements.txt
```

---

## Setup

```bash
# 1. Create virtual environment
python -m venv .venv
.venv\Scripts\activate          # Windows
# source .venv/bin/activate     # macOS/Linux

# 2. Install
pip install -e ".[full,train,dev]"

# 3. Smoke test (tiny model, 50 steps, CPU-safe)
python train_baseline.py --smoke-test

# 4. Run tests
pytest tests/ -v
```

---

## Quick Start

```python
import torch
from sentium import Sentium, SentiumConfig

# Phase 0: clean 200M baseline
cfg   = SentiumConfig.baseline_200m()
model = Sentium(cfg)
print(model)

# Forward pass
ids = torch.randint(0, cfg.vocab_size, (1, 128))
out = model(ids, labels=ids)
print(f"Loss: {out.loss.item():.4f}")

# Phase 1: operator attention + geometric embeddings
cfg1  = SentiumConfig.operator_core()
model1 = Sentium(cfg1)

# Phase 2: + OT MoE routing
cfg2  = SentiumConfig.full_moe()
model2 = Sentium(cfg2)
```

---

## Training

```bash
# Full 200M baseline training
python train_baseline.py --device cuda --out-dir checkpoints/phase0

# With custom config
python train_baseline.py --config my_config.json
```

---

## Evaluation

```python
from sentium import Sentium, SentiumConfig
from sentium.eval import run_full_benchmark

model  = Sentium(SentiumConfig.baseline_small())
report = run_full_benchmark(
    model,
    seq_lens=[512, 1024, 2048, 4096],
    device="cuda",
)
```

---

## Theoretical Foundations

| Component             | Mathematical Basis                                                    | Reference                                      |
| --------------------- | --------------------------------------------------------------------- | ---------------------------------------------- |
| Operator Attention    | Integral operators in function space: $(Kf)(x) = \int K(x,y)f(y)d\mu$ | Fourier Neural Operator (Li et al. 2021)       |
| NystrÃ¶m Approximation | $K \approx K_{qm} K_{mm}^{-1} K_{mk}$                                 | NystrÃ¶mformer (Xiong et al. 2021)              |
| Hyperbolic Embedding  | PoincarÃ© ball model, $\exp_0^c(v)$                                    | Hyperbolic Neural Networks (Ganea et al. 2018) |
| OT Routing            | $\min_\pi \sum c_{ij}\pi_{ij} + \varepsilon H(\pi)$                   | Sinkhorn (Cuturi 2013)                         |
| Stochastic Depth      | $dx = f(x)dt + g(x)dW_t$                                              | Neural SDE (Chen et al. 2021)                  |
| RoPE                  | Complex rotation in embedding space                                   | RoPE (Su et al. 2021)                          |
| SwiGLU                | $\text{FFN}(x) = (W_1 x \odot \text{SiLU}(W_2 x)) W_3$                | GLU Variants (Noam 2020)                       |

---

## Research Roadmap

See [`Project Sentium.md`](./Project%20Sentium.md) for the full research roadmap, risk management, and academic strategy.

---

## Target Platforms

- NVIDIA RTX Ada / Quadro Ada (FP16/BF16/FP8)
- AMD Radeon (high VRAM)
- Apple Silicon Mac Studio (MPS, unified memory)
- High-DDR5 multi-core desktop CPUs

---

*Sentium â€” from sequence modeling to structured modular reasoning.*

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f61290",
   "metadata": {},
   "source": [
    "# ğŸ§  Sentium â€” Colab Training Notebook\n",
    "**Samuel Tanaka Â· Universitas Indonesia Â· February 2026**\n",
    "\n",
    "This notebook trains the **Sentium Phase 0 baseline (454 M params)** on Google Colab using your GPU allocation, with all checkpoints and datasets living in your **2 TB Google Drive**.\n",
    "\n",
    "### Workflow\n",
    "```\n",
    "GitHub repo (code)  â”€â”€â–º  Colab VM (compute)  â”€â”€â–º  Google Drive (data & checkpoints)\n",
    "      â–²                                                      â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€ push from VS Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                           resume next session â—„â”€â”˜\n",
    "```\n",
    "\n",
    "### Sections\n",
    "1. Mount Google Drive\n",
    "2. GPU / runtime check\n",
    "3. Install dependencies\n",
    "4. Clone / pull repo from GitHub\n",
    "5. Configure Drive paths\n",
    "6. Load dataset (HuggingFace streaming â†’ Drive cache)\n",
    "7. Build model\n",
    "8. Configure checkpoint saving to Drive\n",
    "9. Train\n",
    "10. Resume training from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4649ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update -y && sudo apt-get full-upgrade -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b74f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ce08e",
   "metadata": {},
   "source": [
    "## 1 Â· Mount Google Drive\n",
    "Your 2 TB Drive is the persistent storage layer â€” checkpoints survive session restarts, and the dataset cache means you won't re-download every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ede361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "âœ… Drive mounted. Working root: /content/drive/MyDrive/Sentium\n",
      "   Free space: âœ… Drive mounted. Working root: /content/drive/MyDrive/Sentium\n",
      "   Free space: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "import os\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/Sentium\"\n",
    "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "print(f\"âœ… Drive mounted. Working root: {DRIVE_ROOT}\")\n",
    "print(f\"   Free space: \", end=\"\")\n",
    "os.system(f\"df -h /content/drive | tail -1 | awk '{{print $4}}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d628576",
   "metadata": {},
   "source": [
    "## 2 Â· GPU / Runtime Check\n",
    "Verify what hardware Colab allocated. **If you see CPU here, go to Runtime â†’ Change runtime type â†’ GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6293ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU: Tesla T4\n",
      "   VRAM: 14.6 GB\n",
      "   CUDA: 12.6\n",
      "   cuDNN: 91002\n",
      "   Tier: unknown / limited\n",
      "Tesla T4, 15360 MiB, 14910 MiB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    vram_gb = props.total_memory / 1024**3\n",
    "    print(f\"âœ… GPU: {props.name}\")\n",
    "    print(f\"   VRAM: {vram_gb:.1f} GB\")\n",
    "    print(f\"   CUDA: {torch.version.cuda}\")\n",
    "    print(f\"   cuDNN: {torch.backends.cudnn.version()}\")\n",
    "    # Colab GPU tier suggestion\n",
    "    if vram_gb >= 40:\n",
    "        tier = \"A100 ğŸš€  (best tier â€” batch=16, no grad-ckpt)\"\n",
    "    elif vram_gb >= 22:\n",
    "        tier = \"L4 âš¡  (great â€” batch=8)\"\n",
    "    elif vram_gb >= 15:\n",
    "        tier = \"T4 âœ”  (standard â€” batch=4)\"\n",
    "    else:\n",
    "        tier = \"unknown / limited\"\n",
    "    print(f\"   Tier: {tier}\")\n",
    "else:\n",
    "    print(\"âŒ No GPU detected. Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader 2>/dev/null || echo \"(nvidia-smi not available)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d53fa",
   "metadata": {},
   "source": [
    "## 3 Â· Install Dependencies\n",
    "`transformers` and `datasets` are needed for real corpus training. `wandb` is optional but recommended for loss curves you can view from your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b4d84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "# PyTorch is pre-installed on Colab â€” only install the extras\n",
    "!pip install -q \\\n",
    "    einops>=0.7.0 \\\n",
    "    transformers>=4.40.0 \\\n",
    "    datasets>=2.18.0 \\\n",
    "    tokenizers>=0.19.0 \\\n",
    "    wandb>=0.17.0 \\\n",
    "    tqdm>=4.66.0 \\\n",
    "    scipy>=1.12.0\n",
    "\n",
    "print(\"âœ… Dependencies installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3882f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jedi>=0.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19449219",
   "metadata": {},
   "source": [
    "## 4 Â· Clone / Update Repository from GitHub\n",
    "The code lives on GitHub. Every time you open a new Colab session, this cell pulls the latest version â€” so your edits from VS Code are instantly available here after a `git push`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca4b7db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Repo already cloned â€” pulling latest changes from 'master'...\n",
      "HEAD is now at a3d6778 fix: use setuptools.build_meta backend, upgrade setuptools in Colab before editable install\n",
      "HEAD is now at a3d6778 fix: use setuptools.build_meta backend, upgrade setuptools in Colab before editable install\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building editable for sentium (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building editable for sentium (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "âœ… Repo ready at /content/Sentia\n",
      "\u001b[33ma3d6778\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m fix: use setuptools.build_meta backend, upgrade setuptools in Colab before editable install\n",
      "\u001b[33m781bb5a\u001b[m fix: replace removed HF dataset IDs in _DATASET_MAP\n",
      "\u001b[33ma1ee999\u001b[m chore: tighten .gitignore, untrack binaries and personal notes\n",
      "âœ… Repo ready at /content/Sentia\n",
      "\u001b[33ma3d6778\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m fix: use setuptools.build_meta backend, upgrade setuptools in Colab before editable install\n",
      "\u001b[33m781bb5a\u001b[m fix: replace removed HF dataset IDs in _DATASET_MAP\n",
      "\u001b[33ma1ee999\u001b[m chore: tighten .gitignore, untrack binaries and personal notes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# â”€â”€â”€ CONFIGURE THIS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GITHUB_USER = \"sammtan\"   # â† change this\n",
    "REPO_NAME   = \"Sentia\"                 # â† change if your repo name differs\n",
    "BRANCH      = \"master\"\n",
    "# If repo is private, use a Personal Access Token (PAT):\n",
    "#   GITHUB_TOKEN = \"ghp_xxxxxxxxxxxx\"\n",
    "#   REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git\"\n",
    "REPO_URL    = f\"https://github.com/{GITHUB_USER}/{REPO_NAME}.git\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "REPO_DIR = f\"/content/{REPO_NAME}\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"ğŸ“¦ Repo already cloned â€” pulling latest changes from '{BRANCH}'...\")\n",
    "    !git -C {REPO_DIR} fetch origin\n",
    "    !git -C {REPO_DIR} reset --hard origin/{BRANCH}\n",
    "else:\n",
    "    print(f\"ğŸ“¦ Cloning {REPO_URL} ...\")\n",
    "    !git clone --branch {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "\n",
    "# Upgrade setuptools first â€” Colab's system version is too old for editable installs\n",
    "!pip install -q --upgrade setuptools wheel\n",
    "\n",
    "# Install the sentium package in editable mode\n",
    "!pip install -q -e {REPO_DIR}[train]\n",
    "\n",
    "# Add repo to path so imports work\n",
    "import sys\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(f\"âœ… Repo ready at {REPO_DIR}\")\n",
    "!git -C {REPO_DIR} log --oneline -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d72e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40286b4e",
   "metadata": {},
   "source": [
    "## 5 Â· Configure Drive Storage Paths\n",
    "All persistent data (datasets cache, checkpoints, logs) goes into **MyDrive/Sentium/** â€” survives session disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e2e5037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive paths configured:\n",
      "  Checkpoints     : /content/drive/MyDrive/Sentium/checkpoints   (0.00 GB used)\n",
      "  Dataset cache   : /content/drive/MyDrive/Sentium/datasets_cache   (0.00 GB used)\n",
      "  Logs            : /content/drive/MyDrive/Sentium/logs   (0.00 GB used)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€ All persistent storage lives here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DRIVE_ROOT       = Path(\"/content/drive/MyDrive/Sentium\")\n",
    "DRIVE_CKPT_DIR   = DRIVE_ROOT / \"checkpoints\"      # .pt files\n",
    "DRIVE_CACHE_DIR  = DRIVE_ROOT / \"datasets_cache\"   # HuggingFace cache\n",
    "DRIVE_LOG_DIR    = DRIVE_ROOT / \"logs\"             # training logs\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Local (VM) checkpoint dir â€” synced to Drive after each save\n",
    "LOCAL_CKPT_DIR   = Path(\"/content/checkpoints\")\n",
    "\n",
    "for d in [DRIVE_CKPT_DIR, DRIVE_CACHE_DIR, DRIVE_LOG_DIR, LOCAL_CKPT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Drive paths configured:\")\n",
    "for name, path in {\n",
    "    \"Checkpoints\": DRIVE_CKPT_DIR,\n",
    "    \"Dataset cache\": DRIVE_CACHE_DIR,\n",
    "    \"Logs\": DRIVE_LOG_DIR,\n",
    "}.items():\n",
    "    size = sum(f.stat().st_size for f in path.rglob(\"*\") if f.is_file())\n",
    "    print(f\"  {name:16s}: {path}   ({size/1024**3:.2f} GB used)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac67e83",
   "metadata": {},
   "source": [
    "## 6 Â· Load Dataset from Drive Cache\n",
    "Uses HuggingFace `datasets` in **streaming mode** â€” data flows token-by-token from the internet and is cached on Drive so subsequent sessions are instant.\n",
    "\n",
    "> **Switch dataset** by changing `DATASET_ID` below.\n",
    ">\n",
    "> | Alias | HuggingFace ID | Size |\n",
    "> |---|---|---|\n",
    "> | `slim_pajama` | `DKYoon/SlimPajama-6B` | ~6 B tokens, 5.5 M rows |\n",
    "> | `the_pile` | `monology/pile-uncopyrighted` | ~800 GB, unrestricted |\n",
    "> | `openwebtext` | `Skylion007/openwebtext` | ~38 GB |\n",
    "> | `wikitext` | `wikitext-103-raw-v1` | ~500 MB (good for smoke tests) |\n",
    ">\n",
    "> You can also pass any raw HuggingFace dataset id (e.g. `\"allenai/c4\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "668efec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: slim_pajama\n",
      "Tokenizer:       gpt2\n",
      "Cache dir:       /content/drive/MyDrive/Sentium/datasets_cache\n",
      "(First run downloads and caches â€” subsequent runs load from Drive instantly)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)  # ensure sentium is importable\n",
    "\n",
    "from train_colab import HFTextDataset, RandomTokenDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# â”€â”€â”€ Dataset config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATASET_ID     = \"slim_pajama\"  # slim_pajama | the_pile | openwebtext | wikitext\n",
    "TOKENIZER_NAME = \"gpt2\"\n",
    "CONTEXT_LEN    = 512            # will be overridden by train_cfg below\n",
    "MAX_SAMPLES    = None           # set an int (e.g. 50_000) to cap stream for testing\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(f\"Loading dataset: {DATASET_ID}\")\n",
    "print(f\"Tokenizer:       {TOKENIZER_NAME}\")\n",
    "print(f\"Cache dir:       {DRIVE_CACHE_DIR}\")\n",
    "print(\"(First run downloads and caches â€” subsequent runs load from Drive instantly)\")\n",
    "# Dataset is created lazily (streaming) â€” actual loading starts during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcb651",
   "metadata": {},
   "source": [
    "## 7 Â· Build Model\n",
    "Auto-selects VRAM-safe `TrainConfig` for whatever GPU Colab gave you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39ac8604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model: sentium-baseline-200m  â€”  454.2 M parameters\n",
      "   d_model=1024, n_layers=24, n_heads=16, d_ff=4096\n",
      "\n",
      "Training config (VRAM=15.0 GB):\n",
      "  batch_size            = 4\n",
      "  grad_accum            = 4\n",
      "  effective_batch       = 16\n",
      "  context_start         = 512\n",
      "  context_end           = 4096\n",
      "  gradient_checkpointing= False\n",
      "  dtype                 = bf16\n",
      "  max_steps             = 100,000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentium import Sentium, SentiumConfig\n",
    "from sentium.train import TrainConfig\n",
    "from train_colab import detect_gpu_vram, vram_safe_train_cfg\n",
    "\n",
    "# â”€â”€ Auto-detect VRAM and pick matching config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "detected_vram = detect_gpu_vram()\n",
    "\n",
    "# â”€â”€â”€ Training config (auto-selected; override below if needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_cfg = vram_safe_train_cfg(\n",
    "    vram_gb=detected_vram,\n",
    "    save_dir=str(LOCAL_CKPT_DIR),\n",
    "    wandb=False,            # set True after wandb.login() below\n",
    ")\n",
    "# â”€â”€ Optional manual overrides â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# train_cfg.max_steps      = 200_000\n",
    "# train_cfg.batch_size     = 8\n",
    "# train_cfg.context_end    = 2048\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Match dataset context length to training config\n",
    "CONTEXT_LEN = train_cfg.context_start\n",
    "\n",
    "# â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model_cfg = SentiumConfig.baseline_200m()\n",
    "model     = Sentium(model_cfg)\n",
    "\n",
    "params_m = model.num_parameters() / 1e6\n",
    "print(f\"âœ… Model: {model_cfg.model_name}  â€”  {params_m:.1f} M parameters\")\n",
    "print(f\"   d_model={model_cfg.d_model}, n_layers={model_cfg.n_layers}, \"\n",
    "      f\"n_heads={model_cfg.n_heads}, d_ff={model_cfg.d_ff}\")\n",
    "print(f\"\\nTraining config (VRAM={detected_vram:.1f} GB):\")\n",
    "print(f\"  batch_size            = {train_cfg.batch_size}\")\n",
    "print(f\"  grad_accum            = {train_cfg.grad_accum}\")\n",
    "print(f\"  effective_batch       = {train_cfg.batch_size * train_cfg.grad_accum}\")\n",
    "print(f\"  context_start         = {train_cfg.context_start}\")\n",
    "print(f\"  context_end           = {train_cfg.context_end}\")\n",
    "print(f\"  gradient_checkpointing= {train_cfg.gradient_checkpointing}\")\n",
    "print(f\"  dtype                 = {train_cfg.dtype}\")\n",
    "print(f\"  max_steps             = {train_cfg.max_steps:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fdc65",
   "metadata": {},
   "source": [
    "## 8 Â· Configure Checkpointing to Drive\n",
    "Every `save_every` steps, a `.pt` file is written locally **and immediately copied to Drive**. The Drive copy is the ground truth â€” if Colab disconnects mid-run, you lose at most one interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f75b36a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'cerebras/SlimPajama-627B' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2626151037.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Build dataset + dataloader (streaming, Drive-cached)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m dataset = HFTextDataset(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdataset_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASET_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONTEXT_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/Sentia/train_colab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_id, seq_len, cache_dir, tokenizer_name, text_column, max_samples)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DATASET_MAP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mds_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DATASET_MAP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cache_dir\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Dataset] Loading '{ds_name}' (split={split}) ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     raise FileNotFoundError(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m                 ) from e\n\u001b[1;32m    979\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 api.hf_hub_download(\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'cerebras/SlimPajama-627B' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": [
    "from train_colab import patch_trainer_drive_sync, latest_checkpoint\n",
    "from sentium.train import Trainer\n",
    "\n",
    "# â”€â”€â”€ Checkpoint interval (steps) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_cfg.save_every  = 500    # save local + Drive every 500 steps\n",
    "train_cfg.keep_last_n = 5      # keep 5 most recent checkpoints on Drive\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Build dataset + dataloader (streaming, Drive-cached)\n",
    "dataset = HFTextDataset(\n",
    "    dataset_id=DATASET_ID,\n",
    "    seq_len=CONTEXT_LEN,\n",
    "    cache_dir=str(DRIVE_CACHE_DIR),\n",
    "    tokenizer_name=TOKENIZER_NAME,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Build trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=loader,\n",
    "    model_config=model_cfg,\n",
    "    train_config=train_cfg,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Patch Drive sync into every save call\n",
    "patch_trainer_drive_sync(trainer, DRIVE_CKPT_DIR)\n",
    "\n",
    "print(f\"âœ… Trainer ready.\")\n",
    "print(f\"   Checkpoint every {train_cfg.save_every} steps â†’ {DRIVE_CKPT_DIR}\")\n",
    "print(f\"   Keeping last {train_cfg.keep_last_n} checkpoints\")\n",
    "\n",
    "# List existing checkpoints on Drive\n",
    "existing = sorted(DRIVE_CKPT_DIR.glob(\"sentium_step*.pt\"))\n",
    "if existing:\n",
    "    print(f\"\\n   Existing Drive checkpoints ({len(existing)}):\")\n",
    "    for p in existing[-5:]:\n",
    "        size_mb = p.stat().st_size / 1024**2\n",
    "        print(f\"     {p.name}  ({size_mb:.0f} MB)\")\n",
    "else:\n",
    "    print(\"\\n   No existing checkpoints on Drive â€” fresh start.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b1355",
   "metadata": {},
   "source": [
    "## 9 Â· Train ğŸš€\n",
    "Run the cell below to start training. Progress logs are printed to this cell **and** appended to `MyDrive/Sentium/logs/sentium_train.log`.\n",
    "\n",
    "> âš ï¸ Colab free tier disconnects after ~90 min idle or ~12 h. The checkpoint synced to Drive on the last `save_every` boundary is fully resumable â€” see Section 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be311a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys, time\n",
    "\n",
    "# Route trainer prints through logging so they appear in both cell output and log file\n",
    "log_path = DRIVE_LOG_DIR / \"sentium_train.log\"\n",
    "handlers = [logging.StreamHandler(sys.stdout),\n",
    "            logging.FileHandler(log_path, mode=\"a\")]\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s | %(message)s\",\n",
    "                    datefmt=\"%H:%M:%S\",\n",
    "                    handlers=handlers, force=True)\n",
    "\n",
    "print(f\"ğŸ“ Log file: {log_path}\")\n",
    "print(f\"ğŸš€ Starting training â€” {model.num_parameters()/1e6:.1f} M params on {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   Max steps: {train_cfg.max_steps:,}  |  Checkpoint every: {train_cfg.save_every}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "t_start = time.time()\n",
    "trainer.train()\n",
    "elapsed = (time.time() - t_start) / 60\n",
    "print(f\"\\nâœ… Training complete in {elapsed:.1f} min. Final step: {trainer.step}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a292a",
   "metadata": {},
   "source": [
    "## 10 Â· Resume Training from Drive Checkpoint\n",
    "Session expired? New Colab session? Run cells 1â€“7 to re-mount Drive, re-clone, re-build model â€” then run this cell to load the latest checkpoint and continue exactly where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2dd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_colab import latest_checkpoint\n",
    "\n",
    "# â”€â”€ Find latest checkpoint on Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ckpt_path = latest_checkpoint(DRIVE_CKPT_DIR)\n",
    "\n",
    "if ckpt_path is None:\n",
    "    print(\"âŒ No checkpoint found in Drive. Run Section 9 first.\")\n",
    "else:\n",
    "    size_mb = ckpt_path.stat().st_size / 1024**2\n",
    "    print(f\"âœ… Found checkpoint: {ckpt_path.name}  ({size_mb:.0f} MB)\")\n",
    "\n",
    "    # Load into trainer (restores model weights, optimizer state, step counter)\n",
    "    trainer.load_checkpoint(ckpt_path)\n",
    "    print(f\"   Resumed from step {trainer.step:,}\")\n",
    "    print(f\"   Remaining steps: {train_cfg.max_steps - trainer.step:,}\")\n",
    "\n",
    "    # â”€â”€ Continue training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nğŸš€ Resuming training...\")\n",
    "    trainer.train()\n",
    "    print(f\"\\nâœ… Done. Final step: {trainer.step:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2d20f",
   "metadata": {},
   "source": [
    "---\n",
    "## 11 Â· (Optional) Weights & Biases â€” Monitor Loss from Your Laptop\n",
    "W&B lets you watch training curves in real-time from any browser, even while Colab is running on Google's machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Login once â€” stores key in Colab's secret manager for the session\n",
    "wandb.login()   # will prompt for API key; get one free at wandb.ai\n",
    "\n",
    "# Then enable W&B in train_cfg and re-run Section 8+9:\n",
    "train_cfg.use_wandb   = True\n",
    "train_cfg.project_name = \"sentium\"\n",
    "print(\"âœ… W&B enabled. Training metrics will stream to wandb.ai/your-username/sentium\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c37bb",
   "metadata": {},
   "source": [
    "---\n",
    "## 12 Â· (Optional) Smoke Test â€” Verify Everything Works Before Full Run\n",
    "Runs 50 steps on random data with a tiny model. Should complete in < 30 seconds on any GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0456ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {REPO_DIR}/train_colab.py \\\n",
    "    --smoke-test \\\n",
    "    --drive-dir \"{DRIVE_ROOT}\" \\\n",
    "    --no-drive\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

@article{vaswani2017attention,
  title   = {Attention Is All You Need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {30},
  year    = {2017}
}

@article{katharopoulos2020transformers,
  title   = {Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention},
  author  = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  journal = {International Conference on Machine Learning},
  year    = {2020}
}

@article{beltagy2020longformer,
  title   = {Longformer: The Long-Document Transformer},
  author  = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  journal = {arXiv preprint arXiv:2004.05150},
  year    = {2020}
}

@article{zaheer2020bigbird,
  title   = {{BigBird}: Transformers for Longer Sequences},
  author  = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  year    = {2020}
}

@article{xiong2021nystromformer,
  title   = {Nystr{\"{o}}mformer: A {Nystr{\"{o}}m}-Based Self-Attention Mechanism},
  author  = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  journal = {AAAI Conference on Artificial Intelligence},
  year    = {2021}
}

@article{dao2022flashattention,
  title   = {{FlashAttention}: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  year    = {2022}
}

@article{dao2023flashattention2,
  title   = {{FlashAttention-2}: Faster Attention with Better Parallelism and Work Partitioning},
  author  = {Dao, Tri},
  journal = {International Conference on Learning Representations},
  year    = {2024}
}

@article{nickel2017poincare,
  title   = {Poincar{\'e} Embeddings for Learning Hierarchical Representations},
  author  = {Nickel, Maximilian and Kiela, Douwe},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {30},
  year    = {2017}
}

@article{ganea2018hyperbolic,
  title   = {Hyperbolic Neural Networks},
  author  = {Ganea, Octavian-Eugen and B{\'e}cigneul, Gary and Hofmann, Thomas},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {31},
  year    = {2018}
}

@article{kipf2016semi,
  title   = {Semi-Supervised Classification with Graph Convolutional Networks},
  author  = {Kipf, Thomas N. and Welling, Max},
  journal = {International Conference on Learning Representations},
  year    = {2017}
}

@article{shazeer2017outrageously,
  title   = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author  = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc V. and Hinton, Geoffrey E. and Dean, Jeff},
  journal = {International Conference on Learning Representations},
  year    = {2017}
}

@article{fedus2022switch,
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author  = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal = {Journal of Machine Learning Research},
  volume  = {23},
  number  = {120},
  pages   = {1--39},
  year    = {2022}
}

@inproceedings{lewis2021base,
  title   = {{BASE} Layers: Simplifying Training of Large, Sparse Models},
  author  = {Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  booktitle = {International Conference on Machine Learning},
  year    = {2021}
}

@article{cuturi2013sinkhorn,
  title   = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
  author  = {Cuturi, Marco},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {26},
  year    = {2013}
}

@inproceedings{huang2016deep,
  title   = {Deep Networks with Stochastic Depth},
  author  = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.},
  booktitle = {European Conference on Computer Vision},
  year    = {2016}
}

@article{dehghani2018universal,
  title   = {Universal Transformers},
  author  = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal = {International Conference on Learning Representations},
  year    = {2019}
}

@article{chen2018neural,
  title   = {Neural Ordinary Differential Equations},
  author  = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {31},
  year    = {2018}
}

@article{li2020scalable,
  title   = {Scalable Gradients for Stochastic Differential Equations},
  author  = {Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky T. Q. and Duvenaud, David},
  journal = {International Conference on Artificial Intelligence and Statistics},
  year    = {2020}
}

@article{gu2022efficiently,
  title   = {Efficiently Modeling Long Sequences with Structured State Spaces},
  author  = {Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal = {International Conference on Learning Representations},
  year    = {2022}
}

@article{gu2023mamba,
  title   = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author  = {Gu, Albert and Dao, Tri},
  journal = {arXiv preprint arXiv:2312.00752},
  year    = {2023}
}

@article{peng2023rwkv,
  title   = {{RWKV}: Reinventing {RNN}s for the Transformer Era},
  author  = {Peng, Bo and Alcaide, Eric and Anthony, Quentin and others},
  journal = {Findings of EMNLP},
  year    = {2023}
}

@article{su2024roformer,
  title   = {{RoFormer}: Enhanced Transformer with Rotary Position Embedding},
  author  = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal = {Neurocomputing},
  volume  = {568},
  pages   = {127063},
  year    = {2024}
}

@article{ainslie2023gqa,
  title   = {{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author  = {Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zelasko, Yury and Sanghai, Sumit and Xu, Yuhui},
  journal = {Empirical Methods in Natural Language Processing},
  year    = {2023}
}

@article{shazeer2020glu,
  title   = {{GLU} Variants Improve Transformer},
  author  = {Shazeer, Noam},
  journal = {arXiv preprint arXiv:2002.05202},
  year    = {2020}
}

@article{loshchilov2019decoupled,
  title   = {Decoupled Weight Decay Regularization},
  author  = {Loshchilov, Ilya and Hutter, Frank},
  journal = {International Conference on Learning Representations},
  year    = {2019}
}

@article{chen2016training,
  title   = {Training Deep Nets with Sublinear Memory Cost},
  author  = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal = {arXiv preprint arXiv:1604.06174},
  year    = {2016}
}

@article{larsson2017fractalnet,
  title   = {{FractalNet}: Ultra-Deep Neural Networks without Residuals},
  author  = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
  journal = {International Conference on Learning Representations},
  year    = {2017}
}

@article{dong2021attention,
  title   = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth},
  author  = {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  journal = {International Conference on Machine Learning},
  year    = {2021}
}

@article{schulz1933iterative,
  title   = {Iterative {Berechnung} der reziproken {Matrix}},
  author  = {Schulz, G.},
  journal = {ZAMM -- Zeitschrift f{\"u}r Angewandte Mathematik und Mechanik},
  volume  = {13},
  number  = {1},
  pages   = {57--59},
  year    = {1933}
}

@article{shaham2022scrolls,
  title   = {{SCROLLS}: Standardized {CompaRison} Over Long Language Sequences},
  author  = {Shaham, Uri and Segal, Elad and Reid, Maor and others},
  journal = {Empirical Methods in Natural Language Processing},
  year    = {2022}
}

@article{bai2023longbench,
  title   = {{LongBench}: A Bilingual, Multitask Benchmark for Long Context Understanding},
  author  = {Bai, Yushi and Lv, Xin and Zhang, Jiajie and others},
  journal = {arXiv preprint arXiv:2308.14508},
  year    = {2023}
}

@article{chen2021evaluating,
  title   = {Evaluating Large Language Models Trained on Code},
  author  = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and others},
  journal = {arXiv preprint arXiv:2107.03374},
  year    = {2021}
}

@article{jimenez2023swe,
  title   = {{SWE-bench}: Can Language Models Resolve Real-World {GitHub} Issues?},
  author  = {Jimenez, Carlos E. and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal = {International Conference on Learning Representations},
  year    = {2024}
}

@article{zhang2022opt,
  title   = {{OPT}: Open Pre-trained Transformer Language Models},
  author  = {Zhang, Susan and Roller, Stephen and Goyal, Naman and others},
  journal = {arXiv preprint arXiv:2205.01068},
  year    = {2022}
}
